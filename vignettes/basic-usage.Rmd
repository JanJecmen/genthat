---
title: "testr - Basic Usage"
author: "Roman Tsegelskyi & Petr Maj"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

testR is a framework for tests generation from source code and their filtering based on C and R code coverage using the covr package. Testr uses the format of the popular testthat package for the tests it generates. 

Unittests nicely complement the more broaded system testing by concentrating on a single element of the application (usually function) and testing it independently of the others. While this is not always technically possible (due to shared state, global variables, too complex arguments, etc.) many functions usually depend to a great extent only on their arguments. These small, finely targetted tests are ideal for fixing bugs because the failing unittest usually locates precisely the error in the application.

However, creating unittests by hand is time consuming and error prone because of lots of code repetition involved in sending the same, or similar arguments to the functions with only minimal variation over the different tests. testR offers a different approach - given the functions you want to create unit tests for, and a code that actually uses them (this may be vignettes, examples, more broader system tests, or even application code), testR executes the code and records the input arguments to the functions. From these inputs finely targetted tests can then be generated automatically without user intervention. 

Common problem of such tests is that since testR captures *every* function invocation, there may be a lot of duplicities, or invocations that use the function in the same way (so having only one of tests from such group would suffice). TestR therefore offers an option to filter the generated tests to do precisely this - keep only those tests that actually test some yet untested part of the function.

This is accomplished using a so called *code coverage*. Code coverage analyzes which parts of the code have been executed and which have been not. testR inspects the code coverage of the package for existing tests, and then runs the newly generated tests one by one, inspecting the code coverage after each new test. Only tests which actually execute some previously unused part of the application are kept. 

It is important to stress that testR does not actually create new tests from thin air, it just extracts the smallest possible test fragments from existing cod to be used as tests. Therefore the tests produced are only (a) as good as the the code they have been taken from and (b) the tests cannot determine whether the program behaves *correctly*, only whether it behaves *consistently* for a period of time. 

## Creating tests for the first time 

Assume you already have a package with vignettes, examples, and some testthat tests that test the package in general, but you want to have fine grained unittests as well. You can use testR to run the existing code of the package (vignettes, examples and perhaps the tests as well) and generate fine grained tests for these captures. If your package is located in say `"~/mypackage"`, you can execute the following to generate the tests:

```R
testr_package("~/mypackage", verbose = T)
```

By setting verbose to `TRUE`, the function will actually output what it does, which would like similar to this:

```
Building package ~/mypackage
  built into ~/mypackage
Installing package /home/mypackage_0.0.0.9000.tar.gz 
Package mypackage installed
```
First, the package is built and installed. If you prefer to use already built version of the package, you can add the `build = FALSE` as an argument to the `testr_package` function. The package is then reinstalled. 

```
All functions from package will be decoratedDecorating 3 functions
Tracing function "foo" in package "mypackage"
Tracing function "bar" in package "mypackage"
Tracing function "foobar" in package "mypackage"
```

After the package is installed, testR starts tracking all functions in the package (for this one they are aptly named `foo`, `bar` and `foobar`). 

```
Running examples ( 3 files)
Running vignettes ( 2 files)
```

testR then runs the examples and vignettes of the package and if their code calls any of the package's functions, testR will record the input arguments and the result the function gave. If you want, you can run tests too - this would later generate the small grained tests from the exissting unit tests as well. While these will not help catch more bugs, they may help pinpointing what is wrong if the existing tests are rather large. If you want to execute existing tests as well, add optional argument `include.tests = TRUE`. 

```
Generating tests to temp 
Output: temp 
Root: capture 
```

After the code is executed, tests from it are generated into the temporary directory. The temporary directory will be deleted afterwards, but if you want to keep all the tests, you can pass the `output` argument to point to the folder when you want testr to dump the tests. 

```
Filtering tests - this may take some time...
Test cases root -  temp 
Number of test cases -  3 
Test case  temp/mypackage___bar/test-0.R  didn't increase coverage
Test case  temp/mypackage___foo/test-0.R  increased coverage
Test case  temp/mypackage___foo/test-1.R  didn't increase coverage
```
And finally, the tests are filtered against the existing ones (unless `include.tests` is `TRUE`) and each other and only those increasing the code coverage are kept. The final tests will be added to the testthat tests directory for the specified package, one file per function. If you want, you can skip the filtering by passing the `filter = FALSE` argument to the function.

## Adding regressions

Another frequent scenario is that you already have a package, presumably with some tests, and some users. And one of your users finds a bug in your package and send you the code to reproduce the error. Depending on the expertise of the user, this code may be a tiny unittest exposing directly the flaw, but more often this would be a slab of code that does many calls to your package with a simple description that it does not do what it shoud.

In this case you can:

1. Use testR to split the reproducible into small calls to isolated functions in your package and then use this information to quickly localize the error and fix it. Note however, that by the very nature of testR, when you generate the tests, they will *all* pass. Manual inspection of the code is required because you must first identify the problem yourself, then fix the test, and finally fix your code.

2. Fix the problem yourself and then use testR to capture tests from the reproducible code (which now works)

In both cases the test will be kept as a regression so that if this particular functionality will ever break again, you will be left with a siple localized test that fails to quickly see what went wrong.

To create tests from existing code, you can use the `testr_addRegression` function. You submit it the location to the package, the code from which to generate the tests (as a function with no arguments) and other arguments you could have passed to `testr_package` function discussed before:

```R
testr_addRegression(package.dir = ".", code, functions, filter = TRUE, build = TRUE, timed = FALSE, output, verbose = testr_options("verbose"))
```

The output of the function (if `verbose` is enabled) is the same as that of `testr_package` so we refer the readed to that section. 

